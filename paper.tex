\documentclass[]{article}
\renewcommand{\rmdefault}{ptm}
%\usepackage[hmargin=30mm]{geometry}
\usepackage{tabularx}

\begin{document}

\begin{table}[here]
\centering
\begin{tabularx}{\textwidth}{| X | X | }
\hline
Topic of Survey						& 	Cloud Computing\\
\hline
Length of Survey						& 	15 pages\\
\hline
First team member's matriculation number	&	U096833E\\
\hline
First team member's name				&	LAURENCE PUTRA FRANSLAY\\
\hline
Sections done by first member				& 	\\
\hline
Second team member's matriculation number	&	\\
\hline
Second team member's name				&	\\
\hline
Sections done by second member			& 	\\
\hline

\hline
\end{tabularx}
\end{table}

\pagebreak

%9 pages
\section{Introduction to the various methods}
%fill in the method names as the subsection header
\subsection{MapReduce: Simplified Data Processing on Large Clusters}
\subsubsection{Introduction}
MapReduce, made popular by Google, is a programming model that is generally used to process and generate huge sets of data using distributed computing. It is based on 2 core functions of functional languages, \emph{map} and \emph{reduce}. Users of this model will first write a function that \emph{maps} a key-value pair to an intermediate key-value pair, and run this function on all the key-value pairs in the dataset. They will then write another function that aggregates all the values with the same key, and use it to \emph{reduce} the data to the output that is intended. \\

The code that runs the processing on large datasets using this programming model is generally \emph{embarrassingly parallel}\footnotemark\footnotetext{Embarassingly parallel programs are programs that have an almost perfect speed up because each component can be done individually.}, and hence, this programming model is often utilised to process large data sets on multiple computers and sometimes even multiple datacenters. \\

\subsubsection{How it works}
The \emph{map} function, which is written by the user, takes in a set of key-value pair, processes it, and returns a set of intermediate key-value pairs. After the final set of intermediate key-value pair has been generated, the intermediate data set is then passed to the \emph{reduce} function. The \emph{reduce} function will then process all the values with the same key, generating a final result for that key that takes into account all the values that key has.\\

The \emph{map} and \emph{reduce} functions are generally put into a larger MapReduce system that handles more of the backend processing, including allocating the tasks to the machine, and aggregating all the results to return after \emph{reduce} has completed the post processing.\\


\subsubsection{Implementation}
There are various ways to implement \emph{MapReduce}, and it generally depends on the hardware available, and the following describes the implementation that is done at Google.\\

The data is first split into chunks of 16MB to 64MB, dependent on user input, and the program is started up multiple times on the cluster that is assigned for that task. Of all the copies of the program that is started up, one of it would be assigned the master role, and has the job of assigning idle workers to a task. When an idle worker is assigned a \emph{map} task, it will then process the corresponding chunk of data, and pass each key-value pair to the \emph{map} function. The intermediate key-value pairs produced by the \emph{map} function is then temporarily stored in memory. The intermediate key-value pairs stored in memory are then regularly pushed onto local storage, and their stored locations are then passed to the master program to keep track of. The master program will then assign \emph{reduce} tasks to the worker programs, who will sort the intermediate data based on their key's before performing the actual processing. If the amount of data is too large, it will sometimes be done externally by another program. After this processing is completed, the reduce function will process the sorted intermediate data, and return the output and write it to a global storage. Once all the map tasks and reduce tasks are finally completed, the master program will wake up the user program and continue processing in the user program. The result of the \emph{MapReduce} call will be be available in the \emph{n} output files generated by the \emph{n} reduce tasks, and would generally be fed as input to another \emph{MapReduce} function call.\\

As the master program is central to a \emph{MapReduce} operation succeeding, it will have to store the state of each \emph{map} and \emph{reduce} task, as well as the machine the task is running on. The master program also has to store the location and size of the results of each map task that is written to local storage so that the \emph{reduce} tasks are able to find the intermediate data set. \emph{Reduce} tasks that are already running are also given updates when there are any. \\





\subsection{Camdoop}
%Preprocessing instead of post processing

\subsection{Mantri}
% Load balancing tasks and reduce the effects caused by outliers

\subsection{Nectar}

\subsection{Optimising data shuffling in Data Parallel Computation}

%3 pages
\section{Advantages and Disadvantages of each method}
%fill in the method names as the subsection header
\subsection{}

\subsection{}

\subsection{}

\subsection{}

\subsection{}

%1 page
\section{Creating a more robust combination}

%1 page
\section{Conclusion}

\end{document}